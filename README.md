<<<<<<< HEAD
# Spotify Hit Predictor: An MLOps Project ðŸŽ¶

This project applies **Machine Learning Operations (MLOps)** principles to predict whether a song will be a hit or a flop on Spotify based on its audio features. By building an end-to-end MLOps pipeline, I ensured that the model is **robust, scalable, and production-ready**.

---

## ðŸŽ¯ Project Overview

The goal of this project was to build a machine learning model that can classify a song as either a **"hit"** or a **"flop"** by analyzing audio features such as loudness, liveness, danceability, and more.

I structured the entire project using **Object-Oriented Programming (OOP)** principles to maintain **modularity, reusability, and clarity**.

---

## âœ¨ MLOps Pipeline & Key Features

This project goes beyond a standalone machine learning modelâ€”it's designed as a **complete MLOps solution** for real-world deployment.

* **Data Handling & Processing**:
  I implemented modules for **data ingestion, validation, and transformation** to guarantee clean, reliable inputs for model training.

* **Model Training & Optimization**:

  * **MLflow**: Used for **experiment tracking** to log and compare parameters, metrics, and artifacts.
  * **Optuna**: Integrated for **hyperparameter tuning**, automatically searching for the best configurations.

* **CI/CD (Continuous Integration/Continuous Deployment)**:

  * **GitHub Actions** automates the pipeline so that any code push triggers testing, building, and deployment steps.

* **Cloud Infrastructure**:

  * **AWS S3**: Stores the trained models.
  * **AWS ECR**: Hosts Docker images of the application.
  * **AWS EC2**: Serves the application in a scalable cloud environment.

* **API Service**:

  * **FastAPI** powers a REST API that allows real-time predictions. Song features can be sent as JSON requests, and the API responds with predictions.

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ data_ingestion/
â”‚   â”œâ”€â”€ data_transformation/
â”‚   â”œâ”€â”€ data_validation/
â”‚   â”œâ”€â”€ model_evaluation/
â”‚   â””â”€â”€ model_trainer/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data/
â”‚   â””â”€â”€ (Jupyter notebooks for experimentation)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cloud_storage/
â”‚   â”‚   â””â”€â”€ aws_storage.py
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”‚   â”œâ”€â”€ model_pusher.py
â”‚   â”‚   â””â”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ configuration/
â”‚   â”‚   â”œâ”€â”€ aws_connection.py
â”‚   â”‚   â””â”€â”€ mongo_db_connection.py
â”‚   â”œâ”€â”€ constants/
â”‚   â”œâ”€â”€ data_access/
â”‚   â”‚   â””â”€â”€ proj1_data.py
â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â”œâ”€â”€ artifact_entity.py
â”‚   â”‚   â”œâ”€â”€ config_entity.py
â”‚   â”‚   â”œâ”€â”€ estimator.py
â”‚   â”‚   â””â”€â”€ s3_estimator.py
â”‚   â”œâ”€â”€ exception/
â”‚   â”œâ”€â”€ logger/
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ prediction_pipeline.py
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ main_utils.py
â”œâ”€â”€ static/
â”‚   â””â”€â”€ css/
â”‚       â””â”€â”€ style.css
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci_cd.yaml
â”œâ”€â”€ app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ› ï¸ Technologies Used

* **Python**: Core programming language.
* **Scikit-learn, XGBoost**: Machine learning libraries.
* **MLflow**: For tracking experiments.
* **Optuna**: Hyperparameter optimization.
* **FastAPI**: Web framework for serving the model.
* **Docker**: Containerization.
* **GitHub Actions**: CI/CD automation.
* **AWS**:

  * **S3**: Model storage.
  * **ECR**: Docker image registry.
  * **EC2**: Cloud deployment.

---

## âš™ï¸ How to Run the Project

### Prerequisites

* Python 3.8+
* Docker
* AWS account with configured credentials
* GitHub account

### Steps

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/ckswainds/Spotify_tracks_classification.git
   cd spotify-hit-predictor
   ```

2. **Set up AWS Environment**:

   * Create an S3 bucket for storing models.
   * Create an ECR repository for Docker images.
   * Configure AWS credentials.

3. **Local Setup**:

   * Install dependencies:

     ```bash
     pip install -r requirements.txt
     ```
   * Run the training pipeline:

     ```bash
     python main.py
     ```

4. **Deployment (via CI/CD)**:

   * GitHub Actions will automatically:

     1. Build the Docker image.
     2. Push it to AWS ECR.
     3. Deploy it on AWS EC2.

5. **Access the API**:

   * The FastAPI service runs at:
     `http://<EC2_PUBLIC_IP>:8000/predict`
   * Send a `POST` request with song features (JSON) to receive predictions.

---

## ðŸ¤ Contribution

=======
>>>>>>> 4f068a5 (DONE)
